---
title: "Regression-based"
author: "Mihir Kulkarni, Nithika Menon"
date: "2023-12-12"
output: html_document
---


# Part II: Prediction for the Number of Days until an event (death)

Next, let us address our next sub-goal: predicting the number of days a patient survives. The objective is to use the medical test data help doctors make better decisions. To do this, we will be using linear regression.

## Introduction

Linear regression is a statistical technique to model the relationship between a dependent variable and one or more independent variables. The model tries to draw a straight line that best fits the data points on a graph, representing how changes in the independent variables are associated with changes in the dependent variable. The primary advantage of linear regression is its simplicity and ease of interpretation. However, its main drawbacks are that it assumes a linear relationship between variables, which may not always be true. This type of model can be very sensitive to outliers, which affects the model's accuracy, especially dealing with medical data. Furthermore, unlike the decision tree, linear regression models cannot ignore N/A values.

![[Linear Regression Example (]{.underline}[Source](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-linear-regression/))](LinRegEg.png)

## Methodology

Similar to the tree, we will start by splitting the data set into training and testing sets. Note that the data now onluy contains instances where the patient was deceased. The training set will be used to train our model, while the testing set will help evaluate its performance. We'll use 80% of the data for training and the remaining 20% for testing. To allow reproducible code, we have fixed the seed at 380.

```{r}
cirrhosisLRData <- cirrhosis %>%
  filter(Status == "D") %>%
  drop_na()

cirrhosisLRData <- cirrhosisLRData[, -2] #Remove status column


set.seed(380)  #reproducibility

# Sampling 80% 
train_data <- cirrhosisLRData %>% sample_frac(0.8)

# Getting the remaining 20%
test_data <- setdiff(cirrhosisLRData, train_data)

```

### Dataset for Regression
```{r}
cirrhosisRegression <- cirrhosis
```


```{r}

cirrhosisRegression <- cirrhosisRegression %>%
                                        filter(Status != "D") %>%
                                                          mutate(Transplant = ifelse(Status == 'C', 0, 1 ))

```

### Logistic regression model
```{r}

TransplantModel <- glm()


```

=======
### Step 1: Train basic tree

```{r}
LRModel1 <- lm( #train the basic tree
  formula = N_Days ~ Platelets,
  data = test_data,
  na.action = "na.omit"
)

summary(LRModel1)
```

### Step 2: Train basic models

-   fullModel predicts N_Days using all variables from the train_data
-   interceptModel uses only the intercept from the non-missing data
-   initialModel uses only the intercept from the entire dataset.

```{r}
fullModel <- lm(
  formula = N_Days ~ .,
  data = na.omit(train_data)
)

interceptModel <- lm(
  formula = N_Days ~ 1,
  data = na.omit(train_data)
)

initialModel <- lm(
  formula = N_Days ~ 1,
  data = train_data
)
```

##### Calculating RMSE for first model

```{r}
sqrt(mean(residuals(initialModel)^2, na.rm = TRUE))
```

### Step 3: Testing selection models

```{r}
stepwiseTrainModel <- step(
  object = initialModel, 
  scope = list(
    lower = initialModel,
    upper = fullModel
  ),
  data = train_data, 
  direction = "both",
  k = log(nrow(na.omit(train_data))),
  trace = 0
)
```

Here, we will be trying various algorithms to choose the best features/attributes

#### (a) Backward Selection

-   Backward selection is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure.

-   step() function is employed to perform backward selection. The fullModel is used as the starting model, and variables are iteratively removed until the model reaches the specification of interceptModel, which only contains the intercept. The data used for this selection process omits any rows with missing values. The direction = backward argument specifies the backward elimination method, and k = 2 determines the penalty for adding a variable. Finally, trace = 0 indicates that the intermediate steps of the selection process won't be printed. The final model after backward selection is stored in ModelBS

summary of backward:

```{r}
ModelBS <- step(
  object = fullModel,
  scope = list(
    lower = interceptModel,
    upper = fullModel
  ),
  data = na.omit(test_data), 
  direction = "backward",
  k = 2,
  trace = 0
)

# Summary of the final backward selection model ----
summary(ModelBS)
```

#### (b) Forward Selection

-   Forward selection is a type of stepwise regression which begins with an empty model and adds in variables one by one. In each forward step, you add the one variable that gives the single best improvement to your model.
-   The step() function to perform forward model selection. Starting from a basic model (interceptModel), the function iteratively adds predictors from the fullModel if they improve the model fit based on some criterion (in this case, likely AIC with a penalty factor of 2, given by k = 2). The trace = 0 argument suppresses intermediate output, providing a cleaner console output. This technique helps in identifying the most relevant predictors for a regression model in a data-driven manner.

Summary of foward:

```{r}
ModelFS <- step(
  object = interceptModel,
  scope = list(
    lower = interceptModel,
    upper = fullModel
  ),
  data = na.omit(test_data), 
  direction = "forward",
  k = 2,
  trace = 0
)

# Summary of the final forward selection model ----
summary(ModelFS)
```

#### (c) Stepwise

-   Stepwise regression is the step-by-step iterative construction of a regression model that involves the selection of independent variables to be used in a final model. It involves adding or removing potential explanatory variables in succession and testing for statistical significance after each iteration.

-   the step function conducts stepwise selection for a regression model. Initially, it starts with an interceptModel and evaluates the inclusion or exclusion of predictor variables based on their significance and contribution to the model fit. The scope argument specifies the simplest and most complex models. In this case, the simplest is the interceptModel and the most complex is the fullModel. The selection process can both add and remove predictors based on their importance. The penalty for model complexity is determined by the logarithm of the number of observations.

Summary of stepwise:

```{r}
# Use Stepwise Selection ---
ModelSS <- step(
  object = interceptModel, 
  scope = list(
    lower = interceptModel,
    upper = fullModel
  ),
  data = na.omit(test_data), 
  direction = "both",
  k = log(nrow(na.omit(test_data))),
  trace = 0
)

# Summary of the final stepwise selection model ----
summary(ModelSS)
```

```{r}
## Backward Selection testing
sqrt(mean(residuals(stepwiseTrainModel)^2, na.rm = TRUE))
```

## Step 4: Results

### Build QQ Plot for the stepwise Train Model

```{r}

library(car)
car::qqPlot(
  x = residuals(stepwiseTrainModel),
  distribution = "norm",
  id = FALSE,
  envelope = 0.9,
  pch = 20,
  xlab = "Theoretical quantiles",
  ylab = "Residuals (grams)",
  main = "Assess Gaussian Errors"
)
```

### Calculate Skewness of Residuals from the Stepwise Training Model

```{r}
library(psych)
# Sample Skewness ----
skew(residuals(stepwiseTrainModel))
```

#### Sample Excess Kurtosis

```{r}
kurtosi(residuals(stepwiseTrainModel))
```

#### Built a plot for the model

```{r}
ggplot(
  data = data.frame(
    residuals = residuals(stepwiseTrainModel),
    fitted = fitted(stepwiseTrainModel)
  ),
  mapping = aes(x = fitted, y = residuals)
) +
  geom_point() +
  geom_smooth(
    formula = y ~ x,
    method = stats::loess,
    method.args = list(degree = 1),
    se = FALSE,
    linewidth = 0.5
  ) +
  geom_hline(
    yintercept = mean(residuals(stepwiseTrainModel)),
    color = "darkgrey",
    linetype = "solid",
    linewidth = 1
  ) +
  geom_hline(
    yintercept = 0,
    color = "red",
    linetype = "dashed"
  ) +
  theme_bw() +
  labs(
    x = "Fitted values (grams)",
    y = "Residuals (grams)"
  )
```

```{r}
library(car)
library(dplyr)
library(knitr)
library(kableExtra)

# Calculate VIFs
vif_values <- vif(ModelBS)

# Convert to data frame
vif_df <- data.frame(
  Variable = names(vif_values),
  VIF = as.numeric(vif_values)
)

# Add squared VIF column
vif_df <- vif_df %>%
  mutate(
    VIF_Squared = VIF^2
  )

# Display the results
kable(
  x = vif_df,
  digits = 3,
  align = "cccc",
  booktab = TRUE,
  table.attr = 'data-quarto-disable-processing="true"'
) %>%
  kableExtra::kable_classic(
    latex_options = c("HOLD_position"),
    full_width = FALSE
  )

```

### Predictions

```{r}
predictVals <- predict(
  object = stepwiseTrainModel,
  newdata = test_data
)

# Add predictions to data frame and find residuals ----
test_data$prediction <- predictVals
test_data <- test_data %>%
  mutate(
    residuals = N_Days - prediction
  )

# Calculate RMSE for Testing Data ----
rmse_testing <- sqrt(mean(test_data$residuals^2, na.rm = TRUE))
rmse_testing
```

### Plot predictions

the blue reference line indicates where predictions match actual values perfectly.

```{r}
ggplot(
  data = test_data,
  mapping = aes(x = N_Days, y = prediction)
) +
  geom_point() +
  geom_abline( 
    slope = 1,
    intercept = 0,
    color = "blue",
    linetype = "solid",
    linewidth = 1
  ) +
  theme_bw() +
  labs(
    x = "Actual",
    y = "Predicted"
  )
```
